{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATE:13/11/12 ,\n",
    "ASSIGNMENT 2: \n",
    "    SAURAV RAI ,\n",
    "    REGD NO:17558 ,\n",
    "    \" THIS IS A SOFTMAX REGRESSION IMPLEMENTATION IN PYTHON CODE: \"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the libraries required:\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all the data \n",
    "X = loadmat('/home/sauravrai/Desktop/ACADEMICS/semesterII/machine_learning/mac_lab/SoftmaxRegression/Data/mnistTrainImages.mat')\n",
    "X = X['trainData'].T\n",
    "y = loadmat('/home/sauravrai/Desktop/ACADEMICS/semesterII/machine_learning/mac_lab/SoftmaxRegression/Data/mnistTrainLabels.mat')\n",
    "y = y['trainLabels']\n",
    "X_test = loadmat('/home/sauravrai/Desktop/ACADEMICS/semesterII/machine_learning/mac_lab/SoftmaxRegression/Data/mnistTestImages.mat')\n",
    "X_test = X_test['testData'].T\n",
    "y_test = loadmat('/home/sauravrai/Desktop/ACADEMICS/semesterII/machine_learning/mac_lab/SoftmaxRegression/Data/mnistTestLabels.mat')\n",
    "y_test = y_test['testLabels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the values required for our program\n",
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 0.1 # Weight decay parameter\n",
    "options = {'maxiter': 100, 'disp': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The softmax function\n",
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "   \n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    \n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "   \n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    \n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "\n",
    "    return  grad\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The gradient descent method:\n",
    "def gdm(X,y,theta,old_theta,n_classes,input_size,lamda_,data,labels):\n",
    "    # the norm is printed as to see the convergence of our optimization algorithmn\n",
    "    while (np.linalg.norm( old_theta - theta,ord=2)) >= 0.01:  \n",
    "        #print 'norm',np.linalg.norm(old_theta - theta,ord=2)\n",
    "        old_theta=theta\n",
    "        deri=softmax_cost(theta, n_classes, input_size, lambda_, data, labels)\n",
    "        theta = old_theta - lambda_ * deri\n",
    "    return theta       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is for training our model:\n",
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    \"\"\"\n",
    "    Train a softmax model with the given parameters on the given data.\n",
    "    Returns optimal theta, a vector containing the trained parameters\n",
    "    for the model.\n",
    "    input_size: the size of an input vector x^(i)\n",
    "    n_classes: the number of classes\n",
    "    lambda_: weight decay parameter\n",
    "    input_data: an N by M matrix containing the input data, such that\n",
    "                input_data[:, c] is the c-th input\n",
    "    labels: M by 1 matrix containing the class labels for the\n",
    "            corresponding inputs. labels[c] is the class label for\n",
    "            the c-th input\n",
    "    options (optional): options\n",
    "      options['maxiter']: number of iterations to train for\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    old_theta=theta * 99999\n",
    "    J = lambda theta : softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "    # Find out the optimal theta\n",
    "    opt_theta = gdm(X,y,theta,old_theta,n_classes,input_size,lambda_,input_data,labels)\n",
    "    #scipy.optimize.minimize(J, theta, method='L-BFGS-B', jac=True, options=options)\n",
    "    #opt_theta = results['x']\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is for predicting our model\n",
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "   \n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    \n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8505\n"
     ]
    }
   ],
   "source": [
    "model = softmax_train(input_size, n_classes, lambda_, X, y, options)\n",
    "pred = softmax_predict(model, X_test)\n",
    "y_test = y_test.tolist()\n",
    "pred = pred.tolist()\n",
    "print accuracy_score(y_test,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
